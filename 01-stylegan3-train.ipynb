{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27130,"status":"ok","timestamp":1716593498920,"user":{"displayName":"Ted Moore","userId":"02111902761679680911"},"user_tz":240},"id":"WbCGexOebBGi","outputId":"9135ad31-888d-44f0-cf33-060ac7688363"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Connect Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive',force_remount=True)"]},{"cell_type":"markdown","metadata":{"id":"Vn5kRZG8cZFl"},"source":["# StyleGAN3 from NVIDIA\n","\n","**Notes**\n","* To see the original code from NVIDIA [Check here](https://github.com/NVlabs/stylegan3)\n","* We are using a pretrained model and fine-tuning on top of it.\n","* If you come across bugs please post them in [Discord](https://discord.com/invite/awREd7EtMA)\n","\n","---\n","\n","If you find this notebook useful, consider signing up for my [Code Sprout Newsletter](https://codesprout.substack.com/welcome) or [Check my links](https://shyambv.bio.link/)\n","\n","Medium article related to it is mentioned []()"]},{"cell_type":"markdown","metadata":{"id":"L4Y28xIshBYs"},"source":["# Setup"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":91356,"status":"ok","timestamp":1716593590273,"user":{"displayName":"Ted Moore","userId":"02111902761679680911"},"user_tz":240},"id":"jL6aLWrr4zl8","outputId":"4cb4824a-fa95-4844-d7e4-18ab56c92f2e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torch==1.13.0\n","  Downloading torch-1.13.0-cp310-cp310-manylinux1_x86_64.whl (890.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m890.1/890.1 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0) (4.11.0)\n","Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==1.13.0)\n","  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==1.13.0)\n","  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==1.13.0)\n","  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==1.13.0)\n","  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0) (67.7.2)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0) (0.43.0)\n","Installing collected packages: nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.3.0+cu121\n","    Uninstalling torch-2.3.0+cu121:\n","      Successfully uninstalled torch-2.3.0+cu121\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchaudio 2.3.0+cu121 requires torch==2.3.0, but you have torch 1.13.0 which is incompatible.\n","torchtext 0.18.0 requires torch>=2.3.0, but you have torch 1.13.0 which is incompatible.\n","torchvision 0.18.0+cu121 requires torch==2.3.0, but you have torch 1.13.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 torch-1.13.0\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (67.7.2)\n"]}],"source":["# originally was: torch==1.11.0+cu113 torchvision==0.11.1+cu113\n","!pip install torch==1.13.0\n","# torchvision torchaudio -f https://download.pytorch.org/whl/cu113/torch_stable.html\n","!pip install setuptools"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6083,"status":"ok","timestamp":1716593596340,"user":{"displayName":"Ted Moore","userId":"02111902761679680911"},"user_tz":240},"id":"APfJ_U0vx1rH","outputId":"d1186a88-fcee-42d2-8b65-f2867c33e542"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting einops\n","  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ninja\n","  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.1.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.14.0)\n","Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.4)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.2.2)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n","Installing collected packages: ninja, einops\n","Successfully installed einops-0.8.0 ninja-1.11.1.1\n"]}],"source":["!pip install einops ninja gdown"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1716593596340,"user":{"displayName":"Ted Moore","userId":"02111902761679680911"},"user_tz":240},"id":"00S7pPSdhZ5g","outputId":"e0528b85-736b-4e50-d829-3e3d0964c7a1"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/WIP/stylegan3\n"]}],"source":["import os\n","if os.path.isdir('/content/drive/MyDrive/WIP/stylegan3/'):\n","    %cd '/content/drive/MyDrive/WIP/stylegan3/'\n","else:\n","    !git clone https://github.com/NVLabs/stylegan3.git /content/drive/MyDrive/WIP/stylegan3/\n","    %cd '/content/drive/MyDrive/WIP/stylegan3/'\n"]},{"cell_type":"markdown","metadata":{"id":"1XDCK1momzIF"},"source":["# Making a Dataset"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1157321,"status":"ok","timestamp":1716594868572,"user":{"displayName":"Ted Moore","userId":"02111902761679680911"},"user_tz":240},"id":"D45dENn4m1V6","outputId":"fba5bdc1-0466-430f-d1b2-c1ed469f1010"},"outputs":[{"output_type":"stream","name":"stdout","text":["100% 10800/10800 [19:14<00:00,  9.36it/s]\n"]}],"source":["!python /content/drive/MyDrive/WIP/stylegan3/dataset_tool.py \\\n","    --source=/content/drive/MyDrive/dev/arco/240523_01-rose01-YFlips-6min \\\n","    --dest=/content/drive/MyDrive/dev/arco/240523_01-rose01-YFlips-6min.zip \\\n","    --resolution='1024x1024'"]},{"cell_type":"markdown","metadata":{"id":"5YPvBu7plAqg"},"source":["# Model training"]},{"cell_type":"markdown","metadata":{"id":"Vb71nLldlDib"},"source":["You can start from a pre-trained model. Below are some of the models from Nvdia\n","\n","\n","\n","```\n","stylegan3-t-ffhq-1024x1024.pkl, stylegan3-t-ffhqu-1024x1024.pkl, stylegan3-t-ffhqu-256x256.pkl\n","stylegan3-r-ffhq-1024x1024.pkl, stylegan3-r-ffhqu-1024x1024.pkl, stylegan3-r-ffhqu-256x256.pkl\n","stylegan3-t-metfaces-1024x1024.pkl, stylegan3-t-metfacesu-1024x1024.pkl\n","stylegan3-r-metfaces-1024x1024.pkl, stylegan3-r-metfacesu-1024x1024.pkl\n","stylegan3-t-afhqv2-512x512.pkl\n","stylegan3-r-afhqv2-512x512.pkl\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xYAosYKxlGBs"},"source":["## Using pre-trainined model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Dt7NAJ6EYhe","outputId":"77bd174a-c490-4885-cdc3-ee1fcbe41c93"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Training options:\n","{\n","  \"G_kwargs\": {\n","    \"class_name\": \"training.networks_stylegan3.Generator\",\n","    \"z_dim\": 512,\n","    \"w_dim\": 512,\n","    \"mapping_kwargs\": {\n","      \"num_layers\": 2\n","    },\n","    \"channel_base\": 65536,\n","    \"channel_max\": 1024,\n","    \"magnitude_ema_beta\": 0.9997227795604651,\n","    \"conv_kernel\": 1,\n","    \"use_radial_filters\": true\n","  },\n","  \"D_kwargs\": {\n","    \"class_name\": \"training.networks_stylegan2.Discriminator\",\n","    \"block_kwargs\": {\n","      \"freeze_layers\": 0\n","    },\n","    \"mapping_kwargs\": {},\n","    \"epilogue_kwargs\": {\n","      \"mbstd_group_size\": 4\n","    },\n","    \"channel_base\": 32768,\n","    \"channel_max\": 512\n","  },\n","  \"G_opt_kwargs\": {\n","    \"class_name\": \"torch.optim.Adam\",\n","    \"betas\": [\n","      0,\n","      0.99\n","    ],\n","    \"eps\": 1e-08,\n","    \"lr\": 0.0025\n","  },\n","  \"D_opt_kwargs\": {\n","    \"class_name\": \"torch.optim.Adam\",\n","    \"betas\": [\n","      0,\n","      0.99\n","    ],\n","    \"eps\": 1e-08,\n","    \"lr\": 0.002\n","  },\n","  \"loss_kwargs\": {\n","    \"class_name\": \"training.loss.StyleGAN2Loss\",\n","    \"r1_gamma\": 6.6,\n","    \"blur_init_sigma\": 0,\n","    \"blur_fade_kimg\": 50.0\n","  },\n","  \"data_loader_kwargs\": {\n","    \"pin_memory\": true,\n","    \"prefetch_factor\": 2,\n","    \"num_workers\": 3\n","  },\n","  \"training_set_kwargs\": {\n","    \"class_name\": \"training.dataset.ImageFolderDataset\",\n","    \"path\": \"/content/drive/MyDrive/dev/arco/240523_01-rose01-YFlips-6min.zip\",\n","    \"use_labels\": false,\n","    \"max_size\": 10800,\n","    \"xflip\": true,\n","    \"resolution\": 1024,\n","    \"random_seed\": 0\n","  },\n","  \"num_gpus\": 1,\n","  \"batch_size\": 8,\n","  \"batch_gpu\": 4,\n","  \"metrics\": [\n","    \"fid50k_full\"\n","  ],\n","  \"total_kimg\": 500,\n","  \"kimg_per_tick\": 4,\n","  \"image_snapshot_ticks\": 3,\n","  \"network_snapshot_ticks\": 3,\n","  \"random_seed\": 0,\n","  \"ema_kimg\": 2.5,\n","  \"augment_kwargs\": {\n","    \"class_name\": \"training.augment.AugmentPipe\",\n","    \"xflip\": 1,\n","    \"rotate90\": 1,\n","    \"xint\": 1,\n","    \"scale\": 1,\n","    \"rotate\": 1,\n","    \"aniso\": 1,\n","    \"xfrac\": 1,\n","    \"brightness\": 1,\n","    \"contrast\": 1,\n","    \"lumaflip\": 1,\n","    \"hue\": 1,\n","    \"saturation\": 1\n","  },\n","  \"ada_target\": 0.6,\n","  \"resume_pkl\": \"/content/drive/MyDrive/dev/pretrained-models/stylegan3-r-ffhqu-1024x1024.pkl\",\n","  \"ada_kimg\": 100,\n","  \"ema_rampup\": null,\n","  \"run_dir\": \"/content/drive/MyDrive/dev/arco/training/00022-stylegan3-r-240523_01-rose01-YFlips-6min-gpus1-batch8-gamma6.6\"\n","}\n","\n","Output directory:    /content/drive/MyDrive/dev/arco/training/00022-stylegan3-r-240523_01-rose01-YFlips-6min-gpus1-batch8-gamma6.6\n","Number of GPUs:      1\n","Batch size:          8 images\n","Training duration:   500 kimg\n","Dataset path:        /content/drive/MyDrive/dev/arco/240523_01-rose01-YFlips-6min.zip\n","Dataset size:        10800 images\n","Dataset resolution:  1024\n","Dataset labels:      False\n","Dataset x-flips:     True\n","\n","Creating output directory...\n","Launching processes...\n","Loading training set...\n","\n","Num images:  21600\n","Image shape: [3, 1024, 1024]\n","Label shape: [0]\n","\n","Constructing networks...\n","Resuming from \"/content/drive/MyDrive/dev/pretrained-models/stylegan3-r-ffhqu-1024x1024.pkl\"\n","Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n","Setting up PyTorch plugin \"filtered_lrelu_plugin\"... Done.\n","\n","Generator                      Parameters  Buffers  Output shape          Datatype\n","---                            ---         ---      ---                   ---     \n","mapping.fc0                    262656      -        [4, 512]              float32 \n","mapping.fc1                    262656      -        [4, 512]              float32 \n","mapping                        -           512      [4, 16, 512]          float32 \n","synthesis.input.affine         2052        -        [4, 4]                float32 \n","synthesis.input                1048576     3081     [4, 1024, 36, 36]     float32 \n","synthesis.L0_36_1024.affine    525312      -        [4, 1024]             float32 \n","synthesis.L0_36_1024           1049600     157      [4, 1024, 36, 36]     float32 \n","synthesis.L1_36_1024.affine    525312      -        [4, 1024]             float32 \n","synthesis.L1_36_1024           1049600     157      [4, 1024, 36, 36]     float32 \n","synthesis.L2_52_1024.affine    525312      -        [4, 1024]             float32 \n","synthesis.L2_52_1024           1049600     169      [4, 1024, 52, 52]     float32 \n","synthesis.L3_52_1024.affine    525312      -        [4, 1024]             float32 \n","synthesis.L3_52_1024           1049600     157      [4, 1024, 52, 52]     float32 \n","synthesis.L4_84_1024.affine    525312      -        [4, 1024]             float32 \n","synthesis.L4_84_1024           1049600     169      [4, 1024, 84, 84]     float32 \n","synthesis.L5_148_1024.affine   525312      -        [4, 1024]             float32 \n","synthesis.L5_148_1024          1049600     169      [4, 1024, 148, 148]   float16 \n","synthesis.L6_148_1024.affine   525312      -        [4, 1024]             float32 \n","synthesis.L6_148_1024          1049600     157      [4, 1024, 148, 148]   float16 \n","synthesis.L7_276_645.affine    525312      -        [4, 1024]             float32 \n","synthesis.L7_276_645           661125      169      [4, 645, 276, 276]    float16 \n","synthesis.L8_276_406.affine    330885      -        [4, 645]              float32 \n","synthesis.L8_276_406           262276      157      [4, 406, 276, 276]    float16 \n","synthesis.L9_532_256.affine    208278      -        [4, 406]              float32 \n","synthesis.L9_532_256           104192      169      [4, 256, 532, 532]    float16 \n","synthesis.L10_1044_161.affine  131328      -        [4, 256]              float32 \n","synthesis.L10_1044_161         41377       169      [4, 161, 1044, 1044]  float16 \n","synthesis.L11_1044_102.affine  82593       -        [4, 161]              float32 \n","synthesis.L11_1044_102         16524       157      [4, 102, 1044, 1044]  float16 \n","synthesis.L12_1044_64.affine   52326       -        [4, 102]              float32 \n","synthesis.L12_1044_64          6592        25       [4, 64, 1044, 1044]   float16 \n","synthesis.L13_1024_64.affine   32832       -        [4, 64]               float32 \n","synthesis.L13_1024_64          4160        25       [4, 64, 1024, 1024]   float16 \n","synthesis.L14_1024_3.affine    32832       -        [4, 64]               float32 \n","synthesis.L14_1024_3           195         1        [4, 3, 1024, 1024]    float16 \n","synthesis                      -           -        [4, 3, 1024, 1024]    float32 \n","---                            ---         ---      ---                   ---     \n","Total                          15093151    5600     -                     -       \n","\n","Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n","\n","Discriminator  Parameters  Buffers  Output shape         Datatype\n","---            ---         ---      ---                  ---     \n","b1024.fromrgb  128         16       [4, 32, 1024, 1024]  float16 \n","b1024.skip     2048        16       [4, 64, 512, 512]    float16 \n","b1024.conv0    9248        16       [4, 32, 1024, 1024]  float16 \n","b1024.conv1    18496       16       [4, 64, 512, 512]    float16 \n","b1024          -           16       [4, 64, 512, 512]    float16 \n","b512.skip      8192        16       [4, 128, 256, 256]   float16 \n","b512.conv0     36928       16       [4, 64, 512, 512]    float16 \n","b512.conv1     73856       16       [4, 128, 256, 256]   float16 \n","b512           -           16       [4, 128, 256, 256]   float16 \n","b256.skip      32768       16       [4, 256, 128, 128]   float16 \n","b256.conv0     147584      16       [4, 128, 256, 256]   float16 \n","b256.conv1     295168      16       [4, 256, 128, 128]   float16 \n","b256           -           16       [4, 256, 128, 128]   float16 \n","b128.skip      131072      16       [4, 512, 64, 64]     float16 \n","b128.conv0     590080      16       [4, 256, 128, 128]   float16 \n","b128.conv1     1180160     16       [4, 512, 64, 64]     float16 \n","b128           -           16       [4, 512, 64, 64]     float16 \n","b64.skip       262144      16       [4, 512, 32, 32]     float32 \n","b64.conv0      2359808     16       [4, 512, 64, 64]     float32 \n","b64.conv1      2359808     16       [4, 512, 32, 32]     float32 \n","b64            -           16       [4, 512, 32, 32]     float32 \n","b32.skip       262144      16       [4, 512, 16, 16]     float32 \n","b32.conv0      2359808     16       [4, 512, 32, 32]     float32 \n","b32.conv1      2359808     16       [4, 512, 16, 16]     float32 \n","b32            -           16       [4, 512, 16, 16]     float32 \n","b16.skip       262144      16       [4, 512, 8, 8]       float32 \n","b16.conv0      2359808     16       [4, 512, 16, 16]     float32 \n","b16.conv1      2359808     16       [4, 512, 8, 8]       float32 \n","b16            -           16       [4, 512, 8, 8]       float32 \n","b8.skip        262144      16       [4, 512, 4, 4]       float32 \n","b8.conv0       2359808     16       [4, 512, 8, 8]       float32 \n","b8.conv1       2359808     16       [4, 512, 4, 4]       float32 \n","b8             -           16       [4, 512, 4, 4]       float32 \n","b4.mbstd       -           -        [4, 513, 4, 4]       float32 \n","b4.conv        2364416     16       [4, 512, 4, 4]       float32 \n","b4.fc          4194816     -        [4, 512]             float32 \n","b4.out         513         -        [4, 1]               float32 \n","---            ---         ---      ---                  ---     \n","Total          29012513    544      -                    -       \n","\n","Setting up augmentation...\n","Distributing across 1 GPUs...\n","Setting up training phases...\n","Exporting sample images...\n","Initializing logs...\n","2024-05-25 00:52:14.820822: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2024-05-25 00:52:15.660461: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-05-25 00:52:15.660510: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-05-25 00:52:15.764623: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-05-25 00:52:15.965125: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-05-25 00:52:17.121478: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Training for 500 kimg...\n","\n","tick 0     kimg 0.0      time 3m 54s       sec/tick 55.8    sec/kimg 6979.85 maintenance 178.5  cpumem 4.47   gpumem 35.12  reserved 35.86  augment 0.000\n","Evaluating metrics...\n","{\"results\": {\"fid50k_full\": 350.2673321555007}, \"metric\": \"fid50k_full\", \"total_time\": 1703.7691469192505, \"total_time_str\": \"28m 24s\", \"num_gpus\": 1, \"snapshot_pkl\": \"network-snapshot-000000.pkl\", \"timestamp\": 1716600115.6238208}\n","tick 1     kimg 4.0      time 45m 32s      sec/tick 776.1   sec/kimg 194.02  maintenance 1721.5 cpumem 5.77   gpumem 11.32  reserved 14.77  augment 0.036\n","tick 2     kimg 8.0      time 58m 29s      sec/tick 777.0   sec/kimg 194.24  maintenance 0.0    cpumem 5.77   gpumem 11.33  reserved 13.62  augment 0.075\n","tick 3     kimg 12.0     time 1h 11m 27s   sec/tick 777.9   sec/kimg 194.47  maintenance 0.0    cpumem 5.77   gpumem 11.57  reserved 13.62  augment 0.113\n","Evaluating metrics...\n","{\"results\": {\"fid50k_full\": 149.38363836855234}, \"metric\": \"fid50k_full\", \"total_time\": 1568.3809547424316, \"total_time_str\": \"26m 08s\", \"num_gpus\": 1, \"snapshot_pkl\": \"network-snapshot-000012.pkl\", \"timestamp\": 1716604052.0356677}\n","tick 4     kimg 16.0     time 1h 51m 12s   sec/tick 779.9   sec/kimg 194.98  maintenance 1605.4 cpumem 5.77   gpumem 11.45  reserved 13.62  augment 0.150\n","tick 5     kimg 20.0     time 2h 04m 10s   sec/tick 778.3   sec/kimg 194.56  maintenance 0.1    cpumem 5.77   gpumem 11.54  reserved 13.62  augment 0.188\n","tick 6     kimg 24.0     time 2h 17m 09s   sec/tick 778.5   sec/kimg 194.63  maintenance 0.0    cpumem 5.77   gpumem 11.62  reserved 13.62  augment 0.223\n","Evaluating metrics...\n","{\"results\": {\"fid50k_full\": 117.9212911393453}, \"metric\": \"fid50k_full\", \"total_time\": 1567.487035036087, \"total_time_str\": \"26m 07s\", \"num_gpus\": 1, \"snapshot_pkl\": \"network-snapshot-000024.pkl\", \"timestamp\": 1716607965.7076216}\n","tick 7     kimg 28.0     time 2h 56m 25s   sec/tick 779.6   sec/kimg 194.89  maintenance 1576.9 cpumem 5.77   gpumem 11.78  reserved 13.62  augment 0.263\n","tick 8     kimg 32.0     time 3h 09m 27s   sec/tick 781.6   sec/kimg 195.39  maintenance 0.0    cpumem 5.77   gpumem 11.65  reserved 13.62  augment 0.301\n","tick 9     kimg 36.0     time 3h 22m 27s   sec/tick 780.2   sec/kimg 195.04  maintenance 0.1    cpumem 5.77   gpumem 11.52  reserved 13.62  augment 0.340\n","Evaluating metrics...\n","{\"results\": {\"fid50k_full\": 124.55876419776777}, \"metric\": \"fid50k_full\", \"total_time\": 1566.1091258525848, \"total_time_str\": \"26m 06s\", \"num_gpus\": 1, \"snapshot_pkl\": \"network-snapshot-000036.pkl\", \"timestamp\": 1716611886.2006078}\n","tick 10    kimg 40.0     time 4h 01m 47s   sec/tick 780.2   sec/kimg 195.05  maintenance 1579.1 cpumem 5.77   gpumem 11.63  reserved 13.62  augment 0.380\n","tick 11    kimg 44.0     time 4h 14m 47s   sec/tick 780.8   sec/kimg 195.21  maintenance 0.0    cpumem 5.77   gpumem 11.66  reserved 13.62  augment 0.419\n","tick 12    kimg 48.0     time 4h 27m 50s   sec/tick 782.9   sec/kimg 195.73  maintenance 0.0    cpumem 5.77   gpumem 11.74  reserved 13.63  augment 0.457\n","Evaluating metrics...\n","{\"results\": {\"fid50k_full\": 127.53784849878838}, \"metric\": \"fid50k_full\", \"total_time\": 1568.0850508213043, \"total_time_str\": \"26m 08s\", \"num_gpus\": 1, \"snapshot_pkl\": \"network-snapshot-000048.pkl\", \"timestamp\": 1716615807.1789145}\n","tick 13    kimg 52.0     time 5h 07m 10s   sec/tick 782.4   sec/kimg 195.59  maintenance 1577.0 cpumem 5.78   gpumem 11.70  reserved 13.63  augment 0.495\n","tick 14    kimg 56.0     time 5h 20m 14s   sec/tick 783.8   sec/kimg 195.95  maintenance 0.0    cpumem 5.78   gpumem 11.73  reserved 13.63  augment 0.533\n","tick 15    kimg 60.0     time 5h 33m 18s   sec/tick 784.0   sec/kimg 196.00  maintenance 0.0    cpumem 5.78   gpumem 11.73  reserved 13.63  augment 0.572\n","Evaluating metrics...\n","{\"results\": {\"fid50k_full\": 106.83944316744375}, \"metric\": \"fid50k_full\", \"total_time\": 1572.7402675151825, \"total_time_str\": \"26m 13s\", \"num_gpus\": 1, \"snapshot_pkl\": \"network-snapshot-000060.pkl\", \"timestamp\": 1716619739.299298}\n","tick 16    kimg 64.0     time 6h 12m 46s   sec/tick 786.9   sec/kimg 196.71  maintenance 1581.9 cpumem 5.78   gpumem 11.71  reserved 13.63  augment 0.610\n","tick 17    kimg 68.0     time 6h 25m 52s   sec/tick 785.4   sec/kimg 196.36  maintenance 0.1    cpumem 5.78   gpumem 11.84  reserved 13.63  augment 0.646\n","tick 18    kimg 72.0     time 6h 38m 57s   sec/tick 785.0   sec/kimg 196.26  maintenance 0.0    cpumem 5.78   gpumem 11.73  reserved 13.63  augment 0.684\n","Evaluating metrics...\n","{\"results\": {\"fid50k_full\": 93.72116773311105}, \"metric\": \"fid50k_full\", \"total_time\": 1569.9656071662903, \"total_time_str\": \"26m 10s\", \"num_gpus\": 1, \"snapshot_pkl\": \"network-snapshot-000072.pkl\", \"timestamp\": 1716623675.1627672}\n"]}],"source":["# Fine-tune StyleGAN3-R for MetFaces-U using 1 GPU, starting from the pre-trained FFHQ-U pickle.\n","# !python /content/drive/MyDrive/WIP/stylegan3/train.py --help\n","resume_from = '/content/drive/MyDrive/dev/pretrained-models/stylegan3-r-ffhqu-1024x1024.pkl'\n","# resume_from = '/content/drive/MyDrive/dev/arco/training/00020-stylegan3-r-rose01-blobs-square-gpus1-batch8-gamma6.6/network-snapshot-000040.pkl'\n","!python /content/drive/MyDrive/WIP/stylegan3/train.py \\\n","    --outdir=/content/drive/MyDrive/dev/arco/training \\\n","    --cfg=stylegan3-r \\\n","    --data /content/drive/MyDrive/dev/arco/240523_01-rose01-YFlips-6min.zip \\\n","    --gpus=1 --batch=8 --batch-gpu=4 --gamma=6.6 --mirror=1 --kimg=500 --snap=3 \\\n","    --resume {resume_from}\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[{"file_id":"1Nal3M-wjv6BeIgyTgvxhaccPFGEP6cbk","timestamp":1675278682956}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}